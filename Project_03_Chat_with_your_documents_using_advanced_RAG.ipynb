{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sentongo-web/LLMS/blob/main/Project_03_Chat_with_your_documents_using_advanced_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 03 - Chat with your documents using advanced RAG\n",
        "\n",
        "> In this project, we will learn how to create a more advanced RAG pipeline that can:\n",
        "* ask questions about a document that has been read, as if it were a chat with the file itself.\n",
        "* consult more than one reference at the same time.\n",
        "* understand the context of past messages, using the conversation history as a reference to formulate the response\n",
        "\n",
        "And an interface will also be built for this application.\n",
        "Therefore, we can reuse part of the code from the previous project and add new features."
      ],
      "metadata": {
        "id": "CInd-FXyVoZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [ ! ] How to run locally\n",
        "To run the code for this project locally, follow the instructions to install the required dependencies using the commands below. You can use the same installation commands."
      ],
      "metadata": {
        "id": "2Khq09Wegw2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation and Configuration\n",
        "\n",
        "Here we will first load all the functions we used in the previous project and a few others. Among them, FAISS (a vectorstore in the same style as Chroma, which we used in previous classes on RAG) and also other functions necessary for implementing a RAG pipeline that understands the context of conversations.\n",
        "\n",
        "Remember: we can reuse part of the code we created in project 02.\n",
        "So if you want, you can make a copy and make the modifications from there.\n"
      ],
      "metadata": {
        "id": "EqT4YzGtWZGq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEG8KS746Wmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1a0430-8e58-4cf3-fb87-d89791e123a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m422.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit langchain\n",
        "!pip install -q langchain_community langchain-huggingface langchain_ollama langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FAISS Installation\n",
        "\n",
        "Before importing, we need to install FAISS. It is not installed by default in Colab. Therefore, we can use the same command here and locally to install it:\n",
        "\n",
        "`pip install -q faiss-cpu`\n",
        "\n",
        "You can also install `faiss-gpu` if you want to use the GPU-optimized version. For the sake of simplicity, we will use the default CPU version.\n",
        "\n"
      ],
      "metadata": {
        "id": "tv4u-p4J0l9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-cpu"
      ],
      "metadata": {
        "id": "j-hJoWdJEBmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "791d99e4-4aa7-4a05-a0ba-1932999d0751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then use `import faiss` in your application and also import `FAISS` inside langchain library"
      ],
      "metadata": {
        "id": "paEkaqQI0o7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "MP19iHfeEETw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing PyPDFLoader\n",
        "\n",
        "We will use PyPDFLoader to read PDF files in our application. This will be explained in detail in the appropriate section.\n",
        "To use it, we first need to install the library with the command below."
      ],
      "metadata": {
        "id": "ULIGaJSU0qca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "jf2iNG4hEGo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b77048b-bb15-4733-c38e-9ed4c664a4fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other installations\n",
        "\n",
        "Just like in the previous project, we will install dotenv again (in a local environment there is no need to run the installation again, but here in Colab, since it is a new session, we need it) and also localtunnel (remember that this is not necessary in a local environment)."
      ],
      "metadata": {
        "id": "yUTV--pa0tHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-dotenv\n",
        "!npm install -q localtunnel"
      ],
      "metadata": {
        "id": "xo38vjjlEJb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9fb69d-358f-4f90-b4b6-6b3d76788a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 2s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .env\n",
        "HUGGINGFACE_API_KEY=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\n",
        "HUGGINGFACEHUB_API_TOKEN=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\n",
        "OPENAI_API_KEY=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\n",
        "TAVILY_API_KEY=##########\n",
        "SERPAPI_API_KEY=##########\n",
        "LANGCHAIN_API_KEY=##########"
      ],
      "metadata": {
        "id": "uHxWcGDqEQZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3209f2e-29c3-4a86-ebd6-1e43bbc792f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing .env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (code explanations - step by step)\n",
        "\n",
        "First, we will do all the necessary imports"
      ],
      "metadata": {
        "id": "Fbzg8f-gZNDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the sidebar in the interface\n",
        "\n",
        "Let's create an element in our interface where we will upload files. In this case, we will create a sidebar, where there will be a field to upload PDF files to be processed in the application.\n",
        "\n",
        "If no file is uploaded, a message is displayed asking the user to upload a file and interrupts execution until a file is uploaded.\n",
        "\n",
        "We can add this code snippet right after the declared functions\n",
        "\n",
        "Explanations:\n",
        "\n",
        "* `uploads = st.sidebar.file_uploader(...)` - Creates a file upload component in the interface's sidebar, allowing the user to upload multiple PDF files. Here we specify the parameter to accept multiple documents and limit it to PDFs only (at the moment we are interested in programming to accept only this extension, but later we can increase support, allowing us to accept an infinite number of different extensions)\n",
        "\n",
        "* `if not uploads:` - Checks if no file was uploaded.\n",
        "\n",
        "* `st.info` - displays the message in the interface\n",
        "* `st.stop()` - Stops the execution of the code after the message, preventing the rest of the application from running without files loaded."
      ],
      "metadata": {
        "id": "VISve571ZOtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Creates side panel in the interface\n",
        "uploads = st.sidebar.file_uploader(\n",
        "    label=\"Upload files\", type=[\"pdf\"],\n",
        "    accept_multiple_files=True\n",
        ")\n",
        "if not uploads:\n",
        "    st.info(\"Please send some file to continue!\")\n",
        "    st.stop()"
      ],
      "metadata": {
        "id": "1MQWMhzdaPCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline for Indexing and Retrieval\n",
        "\n",
        "This part basically consists of the indexing and retrieval steps that we learned previously.\n",
        "\n",
        "Just to remind you:\n",
        "\n",
        "1. Load the content of the PDF or other file/media/website/etc.\n",
        "2. Division into Chunks: divide all the contents of the documents into small pieces, or chunks.\n",
        "3. Storage and Transformation into Embeddings: These chunks are transformed into embeddings, which are vector representations of the texts. The embeddings are stored in a vector database.\n",
        "4. Use of Retriever: The vector database provides a retriever that searches for the most relevant chunks based on a similarity algorithm.\n",
        "5. Generation: Joining the context to the prompt and generating the final result (the inference will not be placed in this function, so we will do the code later)\n",
        "\n",
        "Therefore, since the indexing and retrieval pipeline is essentially the same as our previous RAG application, we can reuse the code.\n",
        "\n",
        "In the previous project we did everything in the same function (`model_response()`), and in this project we separated it into two (the retriever function that we are seeing now; and the function that will return the chain, as will be shown below). We do this to have greater flexibility as the pipeline is more complex now\n",
        "\n",
        "For the indexing and retrieval pipeline we gathered all of this inside a `config_retriever()` function that will accept the documents sent as a parameter\n",
        "\n",
        "*(explanations below)*"
      ],
      "metadata": {
        "id": "WkBQVXZpaQb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def config_retriever(uploads):\n",
        "    # Load\n",
        "    docs = []\n",
        "    temp_dir = tempfile.TemporaryDirectory()\n",
        "    for file in uploads:\n",
        "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "        with open(temp_filepath, \"wb\") as f:\n",
        "            f.write(file.getvalue())\n",
        "        loader = PyPDFLoader(temp_filepath)\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "    # Split\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
        "\n",
        "    # Store\n",
        "    vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "    vectorstore.save_local('vectorstore/db_faiss')\n",
        "\n",
        "    # Retrieve\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type='mmr',\n",
        "        search_kwargs={'k':3, 'fetch_k':4}\n",
        "    )\n",
        "\n",
        "    return retriever"
      ],
      "metadata": {
        "id": "-IxEOmBra85R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The code here has some differences and additional details compared to the previous one. We will list them below\n",
        "\n",
        "#### - Loading documents (Reading files)\n",
        "\n",
        "This code block below prepares documents to be indexed and used in RAG. The code loads PDFs provided by the user, stores them in a temporary directory, and processes them so that they can be used to create embeddings and retrieve information with retrieval.\n",
        "\n",
        "Previously we used WebBaseLoader to read a web page. Now, since we want to read a PDF, we will use PyPDFLoader. To use it, you need to import `from langchain_community.document_loaders import PyPDFLoader` (it is already in the code block with all the imports, at the beginning of Colab).\n",
        "\n",
        "> Check out other DocumentLoaders here: https://python.langchain.com/docs/integrations/document_loaders/"
      ],
      "metadata": {
        "id": "-xIPAaT9bKtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"docs = []\n",
        "temp_dir = tempfile.TemporaryDirectory()\n",
        "for file in uploads:\n",
        "    temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "    with open(temp_filepath, \"wb\") as f:\n",
        "        f.write(file.getvalue())\n",
        "    loader = PyPDFLoader(temp_filepath)\n",
        "    docs.extend(loader.load())\"\"\""
      ],
      "metadata": {
        "id": "qSVZpiusZTC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### - Choosing an appropriate embedding model\n",
        "\n",
        "* One thing that can significantly improve results is choosing a better or more appropriate embedding model for the language.\n",
        "\n",
        "* In the experiments we conducted, these showed a crucial difference compared to the model we were using in the first examples with RAG, which did not return such good results because this model was not as prepared for our language.\n",
        "\n",
        "* And why might multilingual models be interesting? Not only because you may want to focus on tests with content in other languages, but because the content read may contain texts in multiple languages (that you maybe even doesn't expect), and if the embedding is not done correctly for these languages then the LLM will not return good results. Thus, it is better to develop a RAG pipeline that consumes several sources from different languages.\n",
        "\n",
        "* Therefore, multilingual embedding models can be essential for RAG systems, allowing robust retrieval and generation of information in different languages.\n",
        "* Selecting the right model for your RAG system is a crucial decision that impacts not only the quality of the response, but also resource utilization and scalability. By carefully considering whether it works well for specific languages or tasks, you can find the best one for your needs.    \n",
        "\n",
        "> **Which models to choose**\n",
        "\n",
        "* Open Source Models\n",
        " * The BGE models in HuggingFace are currently considered the best open source embedding models. The BGE model is created by BAAI - Beijing Academy of Artificial Intelligence. BAAI is a private non-profit organization engaged in AI research and development.\n",
        "\n",
        " * In recent benchmarks, the top performing open source model was [BGE-M3](https://huggingface.co/BAAI/bge-m3). The model has the same context length as the OpenAI models (8K), and is approximately 2.2 GB in size.\n",
        "\n",
        " * There are other lighter (or even larger) alternatives such as [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) and its reduced version, [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5).\n",
        "\n",
        " * **`[!]`** We still recommend BGE-M3 at the moment. But if it is too heavy for your setup, you can use one of the two mentioned above, such as BAAI/bge-small-en-v1.5, which may not be as good as those designed for multiple languages, but they tend to work quite well and are a much lighter solution that maintains much of the quality (at least, this was the case in several tests we performed).\n",
        "\n",
        "* Proprietary models\n",
        " * can be an excellent idea if you don't want to bother with this.\n",
        " * There are from OpenAI, Google, Cohere, Anthropic, etc.\n",
        " * For example, for OpenAI, just change to the `OpenAIEmbeddings()` method.\n",
        " * Using proprietary models can be slightly more practical, in exchange for paying very few cents every million tokens (check the pricing page on the company's website)\n",
        "\n",
        "> **How to change the model**\n",
        "\n",
        "* To use it, just replace the HuggingFaceEmbeddings function parameter in our code, from `sentence-transformers/all-mpnet-base-v2` to `BAAI/bge-m3`, or whatever you want.\n",
        "\n",
        "* Note: Here we are reusing the same function that we already know for loading embeddings, but there are alternatives that can even be more efficient depending on the configuration, such as [FastEmbedEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.fastembed.FastEmbedEmbeddings.html), which is based on FastEmbed, from [Qdrant](https://github.com/qdrant), a company who is a reference in AI and vector stores.\n",
        " * We keep it the same as Hugging Face to avoid installing yet another library and because it also tends to be fast, but if you notice that embedding could be faster (which will be more noticeable if you are loading very large text files) then you can test this function, just install it and change the method, doing its import first (check [documentation](https://python.langchain.com/docs/integrations/text_embedding/fastembed/)).\n",
        "\n",
        "> **Where to find more models**\n",
        "\n",
        "* search for `multilingual` (or by searching the name of your desired language) in hugging face, within the sentence-similarity models category\n",
        "\n",
        "* https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=trending&search=multilingual\n",
        "\n",
        "* https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=trending&search=portuguese\n",
        "\n",
        "* Or in leaderboards, just like we've already seen leaderboards for llms, there are also embedding models. One example is the MTEB Leaderboard on Hugging Face, which provides an up-to-date list of proprietary and open-source embedding models, complete with performance statistics on various tasks such as retrieval and summarization - https://huggingface.co/spaces/mteb/leaderboard\n",
        "\n",
        "`Note:` As with LLMs, it can be important to stay informed about new embedding models to reevaluate and update your choices accordingly.\n",
        "\n",
        "\n",
        "> **More details on open source vs. proprietary**\n",
        "\n",
        "* OpenAI’s recent pricing revision has made access to its API significantly more affordable.\n",
        "\n",
        "* However, cost-effectiveness is not the only factor to consider. Other aspects such as latency, privacy, and control over data processing flows may also be important.\n",
        "\n",
        "* Open-source models offer the advantage of full control over the data, improving privacy and personalization. On the other hand, OpenAI’s API may have latency issues, resulting in long response times.\n",
        "\n",
        "* In short, it is not always a simple choice.\n",
        "\n",
        "* Proprietary solutions can generally be more efficient and interesting for those who prioritize convenience, especially if privacy is not a major concern.\n",
        "\n",
        "* Open-source embedding models are an attractive option due to the advantages discussed above, combining performance with greater control over the data.\n",
        "\n",
        "All the text discussed above regarding this topic is related to the code snippet below"
      ],
      "metadata": {
        "id": "zOe_KyVwbeSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")"
      ],
      "metadata": {
        "id": "WV3Ygwd6beGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### - Choosing a vector database\n",
        "\n",
        "To show an alternative, we will use FAISS instead of the Chroma DB we used before. We will see how easy it is to switch (when there is direct integration with langchain)\n",
        "\n",
        "About FAISS\n",
        "* Facebook AI Similarity Search (FAISS) is a library for efficient similarity search and dense vector clustering.\n",
        "* It contains algorithms that search in sets of vectors of any size, even those that probably do not fit in RAM.\n",
        "* It is a widely chosen option because it is scalable and works well for much larger datasets.\n",
        "* The real advantage of FAISS comes when dealing with large datasets that require fast access to relevant information. It retrieves similar vectors efficiently, which can be very relevant even for other types of systems such as facial recognition that depend on fast data comparison, empowering sectors such as security and surveillance with cutting-edge features.\n",
        "\n",
        "> https://github.com/facebookresearch/faiss\n",
        "\n",
        "Other solutions\n",
        "* Other open source solutions besides [Chroma](https://www.trychroma.com/) and FAISS are for example [Weaviate](https://python.langchain.com/docs/integrations/vectorstores/weaviate/) and [Milvus](https://python.langchain.com/docs/integrations/vectorstores/milvus/).\n",
        "* And there are also other solutions like [VectorstoreIndexCreator](https://api.python.langchain.com/en/latest/indexes/langchain.indexes.vectorstore.VectorstoreIndexCreator.html) from LangChain, which is less scalable but may be more practical for some. See the LangChain vector stores page for full details\n",
        "\n",
        "Proprietary solutions:\n",
        "* One option that is recommended in this case is [**Pinecone**](https://www.pinecone.io/), an optimized cloud-based vector database.\n",
        "* Highly scalable due to cloud infrastructure.\n",
        "* May be preferred for its simplicity and ease of use.\n",
        "\n",
        "* Pinecone is designed to efficiently store and retrieve dense vector embeddings, making it ideal for enhancing LLMs with long-term memory and improving their performance in tasks such as natural language processing.\n",
        "* It offers fast data retrieval, ideal for chatbots, and includes a free tier for storing up to 100,000 vectors (check their page because pricing may change in the future).\n",
        "\n",
        "So while these open source vector databases exist, using such a solution can be practical and simple, and can run efficiently on any machine.\n",
        "\n",
        "We will use FAISS because it is open source, but if you want to use Pinecone just go to the website https://www.pinecone.io/ generate a token, add it to the environment variables and load it using Langchain's Pinecone method instead of using the FAISS method (see [documentation here](https://python.langchain.com/docs/integrations/vectorstores/pinecone/) to copy the command)"
      ],
      "metadata": {
        "id": "jkvbGKPTdkn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q faiss-cpu\n",
        "\"\"\"vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "vectorstore.save_local('vectorstore/db_faiss')\"\"\""
      ],
      "metadata": {
        "id": "oWvRsqDAeYfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### - Change in the retriever\n",
        "\n",
        "Just to show how you could use a different retrievr method, we will use a different search algorithm: MMR - Maximum marginal relevance retrieval. It has already been explained in the RAG colab, but as a reminder:\n",
        " * MMR selects by relevance and diversity among the retrieved documents to avoid passing through duplicate context, ensuring a balance between relevance and diversity in the retrieved items.\n",
        "\n",
        "Since we are using MMR, in addition to the `k` parameter (used to define the number of documents returned by the retriever), we also have the option of defining another parameter, `fetch_k`.\n",
        "\n",
        "* This `fetch_k` parameter defines how many documents are retrieved before applying the MMR algorithm (default value: 20).\n",
        "* A higher value of fetch_k (e.g.: 20-50) can generate more diverse and relevant results, since MMR will have more documents to choose from.\n",
        "* However, higher values of fetch_k increase the computational cost, since MMR needs to be applied to a larger set of documents.\n",
        "* The optimal value of fetch_k depends on the size and quality of your vector store, and the desired balance between result quality and performance.\n",
        "\n",
        "* Compared to the `k` parameter (number of documents returned):\n",
        "\n",
        "* A smaller value of k (e.g. 1-5) is ideal when you need a few highly relevant documents, such as in simple Q&A tasks.\n",
        "\n",
        "* A larger value of k (e.g. 10-20) is useful when you want to provide more context or options to the user, such as in a search engine or recommendation system.\n",
        "\n",
        "Therefore, the best values depend on the specific use case and requirements of your application.\n",
        "\n",
        "In general, it is recommended to start with smaller values of k and fetch_k, gradually increasing them as performance and relevance of results improve. Experimenting with different values and monitoring the impact on application behavior can help you find the optimal balance."
      ],
      "metadata": {
        "id": "4C-mpLhyeqo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"retriever = vectorstore.as_retriever(\n",
        "    search_type='mmr',\n",
        "    search_kwargs={'k':3, 'fetch_k':4}\n",
        ")\"\"\""
      ],
      "metadata": {
        "id": "0TP5KAkcfLcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other codes within the function we created for the retriever remain the same as in the previous RAG example, so they were not commented on above.\n",
        "\n",
        "With this, we have completed the indexing and retrieval pipeline. We can return to the function with `return retriever`, since we want to use this function later in our code and thus assign the retriever to a variable.\n",
        "\n",
        "---\n",
        "### Advanced chain for conversation\n",
        "\n",
        "Now we will be making modifications to the chain that we have already implemented previously and are familiar with. We will do it a little differently this time.\n",
        "\n",
        "As we saw in the previous project, the user's query may require context from the conversation to be understood.\n",
        "\n",
        "In many question and answer (Q&A) applications - as is the case with this project now with RAG for conversation with documents - we want to allow the user to have a fluid conversation and to be able to refer to what was recently said, which means that the RAG pipeline needs some kind of \"memory\" of previous messages and some logic to incorporate them into its current thinking.\n",
        "\n",
        "In project 1, we implemented a way to read the history, which worked very well, but now for RAG we need a more advanced pipeline if we want better results.\n",
        "\n",
        "In short, what we need to change in the logic:\n",
        "\n",
        "* Prompt - update our prompt to support message history.\n",
        "\n",
        "* Contextualizing questions - add a subchain that takes the user's last question and reformulates it in the context of the chat history. You can simply think of this as building a new history-aware retriever.\n",
        "\n",
        "In other words, while before we had:\n",
        "* query -> retriever\n",
        "\n",
        "Now we will have:\n",
        "* (query, chat history) -> LLM -> reformulated query -> retriever\n",
        "\n",
        "This way, LLM will be able to understand when a question is asked in the sequence and needs to know what the previous message was in order to understand what it is referring to (these questions asked in the sequence are also called \"Follow up questions\")\n",
        "* For example, if we type \"talk about company XYZ\" and then ask \"when was it founded?\", we want the model to understand that \"it\" refers to \"company XYZ\". And to do this, it reformulates the question based on the history, hence this additional step.\n",
        "* If you really know that you do not need this behavior, then you can use the same RAG pipeline seen previously, creating the chain that way.\n",
        " * But since we want a more advanced example, let's learn this mode now\n",
        "\n",
        "All the code for this advanced chain is shown below, together in the `config_rag_chain()` function\n",
        "\n",
        "🔗 [This diagram here](https://python.langchain.com/v0.2/assets/images/conversational_retrieval_chain-5c7a96abe29e582bc575a0a0d63f86b0.png) basically shows what we are doing in this function.\n",
        "\n",
        "*- Explanations below -*"
      ],
      "metadata": {
        "id": "LC8rW4dwfOE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def config_rag_chain(model_class, retriever):\n",
        "\n",
        "    ### Loading the LLM\n",
        "    if model_class == \"hf_hub\":\n",
        "        llm = model_hf_hub()\n",
        "    elif model_class == \"openai\":\n",
        "        llm = model_openai()\n",
        "    elif model_class == \"ollama\":\n",
        "        llm = model_ollama()\n",
        "\n",
        "    # Prompt definition\n",
        "    if model_class.startswith(\"hf\"):\n",
        "        token_s, token_e = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\", \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
        "    else:\n",
        "        token_s, token_e = \"\", \"\"\n",
        "\n",
        "    # Contextualization prompt\n",
        "    context_q_system_prompt = \"Given the following chat history and the follow-up question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
        "\n",
        "    context_q_system_prompt = token_s + context_q_system_prompt\n",
        "    context_q_user_prompt = \"Question: {input}\" + token_e\n",
        "    context_q_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", context_q_system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", context_q_user_prompt),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Chain for contextualization\n",
        "    history_aware_retriever = create_history_aware_retriever(\n",
        "        llm=llm, retriever=retriever, prompt=context_q_prompt\n",
        "    )\n",
        "\n",
        "    # Q&A Prompt\n",
        "    qa_prompt_template = \"\"\"You are a helpful virtual assistant answering general questions.\n",
        "Use the following bits of retrieved context to answer the question.\n",
        "If you don't know the answer, just say you don't know. Keep your answer concise.\n",
        "Answer in English. \\n\\n\n",
        "    Question: {input} \\n\n",
        "    Context: {context}\"\"\"\n",
        "\n",
        "    qa_prompt = PromptTemplate.from_template(token_s + qa_prompt_template + token_e)\n",
        "\n",
        "    # Configure LLM and Chain for Q&A\n",
        "\n",
        "    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "    rag_chain = create_retrieval_chain(\n",
        "        history_aware_retriever,\n",
        "        qa_chain,\n",
        "    )\n",
        "\n",
        "    return rag_chain"
      ],
      "metadata": {
        "id": "FMngJZ2Igf2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 0) Important comment about tokens and template\n",
        "\n",
        "Just one detail first: this part of the code below was created only to adapt the prompt **if** we are using the hugging face pipeline like HuggingFaceHub, because remember that at the current time this LangChain implementation does not always correctly identify the stop tokens for certain models.\n",
        "\n",
        "If you don't remember, see the 2nd project's Colab.\n",
        "\n",
        "Here we are using Llama 3 so the start tokens (which we defined as `token_s`) and end tokens (`token_e`) are those below, so if you use another model - like Phi 3 - remember to adapt them to the appropriate format. We will concatenate these variables to our prompts\n",
        "\n",
        "In other words: if you load the LLM through the hugging face hub pipeline then it applies the tokens; otherwise, it is not necessary, so these two variables will have no text (which means that the prompt will remain the same and unchanged)."
      ],
      "metadata": {
        "id": "SiTlm5GcgkBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = \"hf_hub\"\n",
        "# ...\n",
        "\n",
        "if model_class.startswith(\"hf\"):\n",
        "    token_s, token_e = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\", \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
        "else:\n",
        "    token_s, token_e = \"\", \"\""
      ],
      "metadata": {
        "id": "6m9DHKB6gsVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) Query reformulation to contextualize\n",
        "\n",
        "First, we need to define a sub-chain that uses previous messages and the last question asked to reformulate the question itself, if it refers to information already mentioned in the history.\n",
        "\n",
        "* To achieve this, first we define the system contextualization prompt in `context_q_system_prompt` that will tell the model to reformulate the question based on the history\n",
        "\n",
        "* We chose this prompt because it is well accepted and recommended by the library authors, we just modified it a little. Even if you speak another language, we recommend to leave this prompt in english because it has a chance of working better,that's because - like it or not - this model may be better in this language (although we are using a modern model that works well in other languages), but you could change the language here too and test different prompts to see which one gives the best result.\n",
        " * Just to mention, another example of a prompt that could be used: \"Given the following conversation and a follow-up question, rephrase the follow-up question to be a standalone question. This is a conversation with a human. Answer the questions you get based on the knowledge you have. If you don't know the answer, just say that you don't, don't try to make up an answer.\"\n",
        "\n",
        "* And we also define the user's prompt. Although it is not essential, here we put \"Question: \" before {input} because in certain tested models this helped reinforce the LLM's better understanding that the following part is the question and therefore this is the part it should reformulate.\n",
        "\n",
        "* Next, we will use a prompt that includes a variable called \"chat_history\", which as we have already seen allows us to insert a list of messages in the prompt using the input key \"chat_history\". These messages will be inserted after the system message and before the user's most recent question."
      ],
      "metadata": {
        "id": "qN0IeFPLhHcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contextualization prompt\n",
        "context_q_system_prompt = \"Given the following chat history and the follow-up question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
        "\n",
        "context_q_system_prompt = token_s + context_q_system_prompt\n",
        "context_q_user_prompt = \"Question: {input}\" + token_e\n",
        "context_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", context_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", context_q_user_prompt),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "q7QV2iwAiKYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We use the helper function `create_history_aware_retriever` to handle cases where the message history is empty. Otherwise, it applies a sequence of `prompt | llm | StrOutputParser() | retriever`.\n",
        "\n",
        "* The create_history_aware_retriever function builds a chain that accepts input and chat_history as input and has the same output format as a retriever."
      ],
      "metadata": {
        "id": "iLiJebBiiPTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Chain for contextualization\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm=llm, retriever=retriever, prompt=context_q_prompt\n",
        ")\"\"\""
      ],
      "metadata": {
        "id": "xqdNWVariUD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In short, this chain adds a reformulation of the input query to our retriever, so that the retrieval incorporates the context of the conversation.\n",
        "\n",
        "With this, we have everything we need to set up our question and answer chain.\n",
        "\n",
        "#### 2) Question and answer chain (Q&A)\n",
        "\n",
        "This is the chain that will return the final answer, which is why it is known by this name in this context.\n",
        "\n",
        "First, we will create the prompt template, taking the opportunity to change the prompt and adapt it to RAG. We can even use the same template we created for the previous RAG example, but here you can customize it as you wish and add or remove instructions in the prompt. As an addition, we added a sentence asking to return in Portuguese, to reinforce the LLM since part of the content read may be in other languages, so we want it to be translated at the end if necessary."
      ],
      "metadata": {
        "id": "LopbY14HicZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_prompt_template = \"\"\"You are a helpful virtual assistant answering general questions.\n",
        "Use the following bits of retrieved context to answer the question.\n",
        "If you don't know the answer, just say you don't know. Keep your answer concise.\n",
        "Answer in English. \\n\\n\n",
        "Question: {input} \\n\n",
        "Context: {context}\"\"\"\n",
        "\n",
        "qa_prompt = PromptTemplate.from_template(token_s + qa_prompt_template + token_e)"
      ],
      "metadata": {
        "id": "t1pBYgg4i05n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain has some functions to facilitate the creation of the chain that we need to do here and that was described above. Instead of using the LCEL syntax seen previously (where each component is separated by `|`), we can use some methods created exactly for this purpose:\n",
        "\n",
        "* `create_stuff_documents_chain` specifies how the context retrieved (via retriever) is fed into a prompt and LLM. In this case, we will \"stuff\" the content (hence \"stuff\" in the function name) in the prompt, that is, we will include all the retrieved context without any summary or other processing.\n",
        " * we will use it here to generate the question and answer chain (`qa_chain`), with input keys `context`, `chat_history` and `input`.\n",
        "\n",
        "* `create_retrieval_chain` - adds the retrieval step and propagates the retrieved context through the chain, providing it together with the final answer.\n",
        " * this chain applies history_aware_retriever and qa_chain in sequence, retaining intermediate outputs (such as retrieved context) for convenience. For input: `input` and `chat_history`; and output: `input`, `chat_history`, `context` and `answer`. The final response from the model will be accessible by answer (more on that later)\n",
        "\n",
        "In other words, this makes it easier to create a document processing chain for Q&A tasks, combining multiple documents into a single context. This function is essential in the context of conversational RAGs, as it allows the model to effectively use the retrieved information together with the user's queries to generate accurate and contextually relevant answers"
      ],
      "metadata": {
        "id": "VdE0NjA3i5rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(\n",
        "    history_aware_retriever,\n",
        "    qa_chain,\n",
        ")\"\"\""
      ],
      "metadata": {
        "id": "UVzSdJ-9jeW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we completed the chain of our advanced RAG, we can return in the function with `return rag_chain`"
      ],
      "metadata": {
        "id": "foDyDK2Ijg67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### User Input and Response Generation\n",
        "\n",
        "We want the model to generate a response only if we upload at least one document. To create it this behavior, we need to modify after `user_query = st.chat_input...`, adding a new condition here: `and uploads is not None`.\n",
        "after `user_query is not None and user_query != \"\"`\n",
        "\n",
        "that is, it will look like this:"
      ],
      "metadata": {
        "id": "GAHR_68ajsz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "if user_query is not None and user_query != \"\" and uploads is not None:\n",
        "  ###\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8jEtW8YLj2P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retrieving information\n",
        "\n",
        "Now we need to call the function that executes all the retriever logic.\n",
        "\n",
        "Let's add this inside `with st.chat_message(\"AI\"):`\n",
        "\n",
        "Here we will call the config_retriever() function and pass the uploaded files as a parameter (`uploads` variable)\n",
        "\n",
        "So first we have to get the retrieved information"
      ],
      "metadata": {
        "id": "YZpwiC2bkA03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"retriever = config_retriever(uploads)\"\"\""
      ],
      "metadata": {
        "id": "n0jrf4pWkET4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pipeline Optimization\n",
        "\n",
        "We will show you a method we created to optimize the response. If you do it the way above, all the indexing and retrieval steps will be executed for each question asked to the AI.\n",
        "\n",
        "To make your pipeline more efficient, we can avoid unnecessary re-execution of the process of dividing into chunks, creating embeddings and storing in the vector database every time a new question is asked.\n",
        "\n",
        "Since the PDF document is not changing between runs, we can configure it to save this list of the processed file names, then we can use it to compare if there was a change - and only re-run the retrieval snippet if there was a change in any document (or if it was removed or added).\n",
        "\n",
        "First, we will create a variable that stores the list of processed file names and the retriever. This will allow you to keep track of the files already processed between runs of the application.\n",
        "\n",
        "To make the mode values persist in the current session, we will use Streamlit's st.session_state, which we have previously used to save the chat history. So, right after the if \"chat_history\", let's add the following:    "
      ],
      "metadata": {
        "id": "MN_O2acHkGxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"docs_list\" not in st.session_state:\n",
        "    st.session_state.docs_list = None\n",
        "\n",
        "if \"retriever\" not in st.session_state:\n",
        "    st.session_state.retriever = None"
      ],
      "metadata": {
        "id": "jTF97khbkUin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating our variables that persist in the session, simply adapt the retriever code so that it works like this:\n",
        "\n",
        "* at each execution, compare the current file list with the list stored in st.session_state. If there are changes (i.e. new files were uploaded or old files were removed), run the chunking and embedding generation process again.\n",
        "\n",
        "* if the file list has not changed, skip the chunking and embedding generation steps and directly use the retriever stored in the session to search for the information.\n",
        "\n",
        "To implement this logic, simply change the code, replacing `retriever = config_retriever(uploads)` with the following method:"
      ],
      "metadata": {
        "id": "MeeJJJFbkXh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"if st.session_state.docs_list != uploads:\n",
        "    print(uploads)\n",
        "    st.session_state.docs_list = uploads\n",
        "    st.session_state.retriever = config_retriever(uploads) \"\"\""
      ],
      "metadata": {
        "id": "lifpnNzNkfef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the RAG chain and generating the response\n",
        "\n",
        "In this first line below, we call the config_rag_chain function, which contains all the logic we defined above. At the end, this chain is assigned to the rag_chain variable.\n",
        "\n",
        "In the second line, the configured RAG chain is invoked using the invoke method, which receives a dictionary with two keys: input (which contains the user's current query, in this case, user_query), and chat_history (which includes the conversation history stored in st.session_state.chat_history). Remember that the conversation history is used to provide additional context, helping the model generate a more relevant and accurate response based on previous interactions.\n",
        "\n",
        "The result of this execution is stored in the result variable, which contains the final response generated by the chain."
      ],
      "metadata": {
        "id": "NP2nfmXNkjxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "rag_chain = config_rag_chain(model_class, st.session_state.retriever)\n",
        "\n",
        "result = rag_chain.invoke({\"input\": user_query, \"chat_history\": st.session_state.chat_history})\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dTtIh8uekjfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the answer\n",
        "\n",
        "The line below extracts the answer generated by the AI - the `result` variable - which contains the result of the execution of the RAG chain. The value of result is a dictionary and the key 'answer' stores the final answer. Therefore, when accessing result['answer'], we directly obtain the text generated by the LLM in response to the query/question we provided\n",
        "\n",
        "And to display it in the interface within the chat we use again `st.write`, or `st.markdown`"
      ],
      "metadata": {
        "id": "Z-rpQOgck3Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"resp = result['answer']\n",
        "st.write(resp)\"\"\""
      ],
      "metadata": {
        "id": "ecUdDxcMlFHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And again here, the `resp` will also be added to the chat history, with `st.session_state.chat_history.append(AIMessage(content=resp))`\n",
        "\n",
        "### Bonus: showing the source of the information obtained\n",
        "\n",
        "This will be useful especially when loading multiple documents\n",
        "\n",
        "Here we can set to display only the first reference, or all of them. The number of sources here is related to the k parameter of vectorstore.as_retriever\n",
        "\n",
        "* `sources = result['context']`: This line extracts the list of documents/sources used by the model from the 'context' field of the result returned by LLM. These documents contain the information that helped generate the response.\n",
        "\n",
        "* `for idx, doc in enumerate(sources):`: a loop that goes through the list of sources documents, enumerating each document. The idx index will be used to number the sources, and doc represents the current document in each iteration.\n",
        "\n",
        "* `source = doc.metadata['source']`: For each document, the line accesses the value associated with the 'source' key within the document's metadata. This value typically contains the path or URL from which the document was loaded.\n",
        "\n",
        "* `file = os.path.basename(source)`: This line extracts the base name of the file (i.e. the file name without the full path) from the source variable. This makes the name more readable for display.\n",
        "\n",
        "* `page = doc.metadata.get('page', 'Page not specified')`: Here, the code attempts to obtain the document's page number from the metadata. If the page is not specified, the default value 'Page not specified' is used.\n",
        "\n",
        "* `ref = f\":link: Source {idx}: *{file} - p. {page}*\"`: This line formats a string representing the source reference. The source index, file name, and page number are displayed, resulting in a reference like \"Source 1: document.pdf - p. 2\".\n",
        "\n",
        "* `print(ref)`: prints the formatted reference to the console, only for debugging or quick viewing.\n",
        "\n",
        "* `with st.popover(ref):`: Starts a popover (interactive visual element) in Streamlit. This popover will be triggered when the user interacts with the displayed source reference. We use the popover component because we find it interesting in this situation, if you want others components you can find them here https://docs.streamlit.io/develop/api-reference/layout\n",
        "\n",
        "* `st.caption(doc.page_content)`: Inside the popover, the page content (extracted from the doc.page_content variable) is displayed as a caption. So, when the user clicks on the popover they will be able to see the page text directly related to the cited source."
      ],
      "metadata": {
        "id": "4tPkTCdJlHyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "sources = result['context']\n",
        "for idx, doc in enumerate(sources):\n",
        "    source = doc.metadata['source']\n",
        "    file = os.path.basename(source)\n",
        "    page = doc.metadata.get('page', 'Page not specified')\n",
        "\n",
        "    ref = f\":link: Source {idx}: *{file} - p. {page}*\"\n",
        "    print(ref)\n",
        "    with st.popover(ref):\n",
        "        st.caption(doc.page_content)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QU4AxZ69loO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, we will use the time library to count how long the generation took."
      ],
      "metadata": {
        "id": "Op2eDd65lr30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "# rest of the code [...]\n",
        "end = time.time()\n",
        "print(\"Time: \", end - start)"
      ],
      "metadata": {
        "id": "_h2gf9Zbl8j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "## Lauching the interface\n",
        "\n",
        "Finally, we gathered all the code into a single script and added the page configuration with st.set_page_config and st.title. Despite other changes in the logic of our application, compared to project 2 we also changed the title and emoji to make the interface more personalized and suitable for the current project, with a look more aligned with the context of this project."
      ],
      "metadata": {
        "id": "gmGl3IJyUB0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile proj03.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "\n",
        "import torch\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "from langchain_huggingface  import HuggingFaceEndpoint\n",
        "\n",
        "import faiss\n",
        "import tempfile\n",
        "import os\n",
        "import time\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Streamlit Settings\n",
        "st.set_page_config(page_title=\"Chat with documents 📚\", page_icon=\"📚\")\n",
        "st.title(\"Chat with documents 📚\")\n",
        "\n",
        "model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n",
        "\n",
        "## Model Providers\n",
        "def model_hf_hub(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", temperature=0.1):\n",
        "  llm = HuggingFaceEndpoint(\n",
        "      repo_id=model,\n",
        "      temperature=temperature,\n",
        "      max_new_tokens=512,\n",
        "      return_full_text=False,\n",
        "      #model_kwargs={\n",
        "      #    \"max_length\": 64,\n",
        "      #    #\"stop\": [\"<|eot_id|>\"],\n",
        "      #}\n",
        "  )\n",
        "  return llm\n",
        "\n",
        "def model_openai(model=\"gpt-4o-mini\", temperature=0.1):\n",
        "    llm = ChatOpenAI(\n",
        "        model=model,\n",
        "        temperature=temperature\n",
        "        # other parameters...\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "def model_ollama(model=\"phi3\", temperature=0.1):\n",
        "    llm = ChatOllama(\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "\n",
        "## Indexing and Retrieval\n",
        "\n",
        "def config_retriever(uploads):\n",
        "    # Load\n",
        "    docs = []\n",
        "    temp_dir = tempfile.TemporaryDirectory()\n",
        "    for file in uploads:\n",
        "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "        with open(temp_filepath, \"wb\") as f:\n",
        "            f.write(file.getvalue())\n",
        "        loader = PyPDFLoader(temp_filepath)\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "    # Split\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
        "\n",
        "    # Store\n",
        "    vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "    vectorstore.save_local('vectorstore/db_faiss')\n",
        "\n",
        "    # Retrieve\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type='mmr',\n",
        "        search_kwargs={'k':3, 'fetch_k':4}\n",
        "    )\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "def config_rag_chain(model_class, retriever):\n",
        "\n",
        "    ### Loading the LLM\n",
        "    if model_class == \"hf_hub\":\n",
        "        llm = model_hf_hub()\n",
        "    elif model_class == \"openai\":\n",
        "        llm = model_openai()\n",
        "    elif model_class == \"ollama\":\n",
        "        llm = model_ollama()\n",
        "\n",
        "    # Prompt definition\n",
        "    if model_class.startswith(\"hf\"):\n",
        "        token_s, token_e = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\", \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
        "    else:\n",
        "        token_s, token_e = \"\", \"\"\n",
        "\n",
        "    # Contextualization prompt\n",
        "    context_q_system_prompt = \"Given the following chat history and the follow-up question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
        "\n",
        "    context_q_system_prompt = token_s + context_q_system_prompt\n",
        "    context_q_user_prompt = \"Question: {input}\" + token_e\n",
        "    context_q_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", context_q_system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", context_q_user_prompt),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Chain for contextualization\n",
        "    history_aware_retriever = create_history_aware_retriever(\n",
        "        llm=llm, retriever=retriever, prompt=context_q_prompt\n",
        "    )\n",
        "\n",
        "    # Q&A Prompt\n",
        "    qa_prompt_template = \"\"\"You are a helpful virtual assistant answering general questions.\n",
        "Use the following bits of retrieved context to answer the question.\n",
        "If you don't know the answer, just say you don't know. Keep your answer concise.\n",
        "Answer in English. \\n\\n\n",
        "    Question: {input} \\n\n",
        "    Context: {context}\"\"\"\n",
        "\n",
        "    qa_prompt = PromptTemplate.from_template(token_s + qa_prompt_template + token_e)\n",
        "\n",
        "    # Configure LLM and Chain for Q&A\n",
        "\n",
        "    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "    rag_chain = create_retrieval_chain(\n",
        "        history_aware_retriever,\n",
        "        qa_chain,\n",
        "    )\n",
        "\n",
        "    return rag_chain\n",
        "\n",
        "\n",
        "## Creates side panel in the interface\n",
        "uploads = st.sidebar.file_uploader(\n",
        "    label=\"Upload files\", type=[\"pdf\"],\n",
        "    accept_multiple_files=True\n",
        ")\n",
        "if not uploads:\n",
        "    st.info(\"Please send some file to continue!\")\n",
        "    st.stop()\n",
        "\n",
        "\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = [\n",
        "        AIMessage(content=\"Hi, I'm your virtual assistant! How can I help you?\"),\n",
        "    ]\n",
        "\n",
        "if \"docs_list\" not in st.session_state:\n",
        "    st.session_state.docs_list = None\n",
        "\n",
        "if \"retriever\" not in st.session_state:\n",
        "    st.session_state.retriever = None\n",
        "\n",
        "for message in st.session_state.chat_history:\n",
        "    if isinstance(message, AIMessage):\n",
        "        with st.chat_message(\"AI\"):\n",
        "            st.write(message.content)\n",
        "    elif isinstance(message, HumanMessage):\n",
        "        with st.chat_message(\"Human\"):\n",
        "            st.write(message.content)\n",
        "\n",
        "# we use time to measure how long it took for generation\n",
        "start = time.time()\n",
        "user_query = st.chat_input(\"Enter your message here...\")\n",
        "\n",
        "if user_query is not None and user_query != \"\" and uploads is not None:\n",
        "\n",
        "    st.session_state.chat_history.append(HumanMessage(content=user_query))\n",
        "\n",
        "    with st.chat_message(\"Human\"):\n",
        "        st.markdown(user_query)\n",
        "\n",
        "    with st.chat_message(\"AI\"):\n",
        "\n",
        "        if st.session_state.docs_list != uploads:\n",
        "            print(uploads)\n",
        "            st.session_state.docs_list = uploads\n",
        "            st.session_state.retriever = config_retriever(uploads)\n",
        "\n",
        "        rag_chain = config_rag_chain(model_class, st.session_state.retriever)\n",
        "\n",
        "        result = rag_chain.invoke({\"input\": user_query, \"chat_history\": st.session_state.chat_history})\n",
        "\n",
        "        resp = result['answer']\n",
        "        st.write(resp)\n",
        "\n",
        "        # show the source\n",
        "        sources = result['context']\n",
        "        for idx, doc in enumerate(sources):\n",
        "            source = doc.metadata['source']\n",
        "            file = os.path.basename(source)\n",
        "            page = doc.metadata.get('page', 'Page not specified')\n",
        "\n",
        "            ref = f\":link: Source {idx}: *{file} - p. {page}*\"\n",
        "            print(ref)\n",
        "            with st.popover(ref):\n",
        "                st.caption(doc.page_content)\n",
        "\n",
        "    st.session_state.chat_history.append(AIMessage(content=resp))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time: \", end - start)"
      ],
      "metadata": {
        "id": "0P5CTavqvh74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39718b73-425d-4072-99fd-c8e641aa7582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing proj03.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Streamlit\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M54-PNNEPrxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run proj03.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "KrnuMrbds4Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now to connect, we use the command below (explanations in colab from project 02)"
      ],
      "metadata": {
        "id": "yYiheBeuN8SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com\n",
        "\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "IMjgsW8c1NvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e1b030-60f2-4ff4-e3b3-ffcfb2f8b93f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.124.176.101\n",
            "your url is: https://weak-pets-send.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Application\n",
        "The first question will take a bit longer because it needs to run all RAG stages, so the time will vary based on the content size. However, the next question should be much faster, as we’ve programmed the application to avoid unnecessary processing (remember, it will only re-run the indexing and retrieval stages if you add or remove a file).\n",
        "\n",
        "**(suggestion of what to type)**\n",
        "\n",
        "1. Upload the file \"BlueNexus Industries Presentation.pdf\" in the side panel (this and the other PDFs mentioned are located in the documents folder on Drive). This is a presentation we created as an example for a fictional company, which contains various details about the company.\n",
        "Type the following messages:\n",
        "\n",
        "* `tell me about BlueNexus`\n",
        "\n",
        "* `when was it founded?`\n",
        "\n",
        "* `what was the revenue for the last year?`\n",
        "\n",
        "* `who is Dr. Watson?`\n",
        "\n",
        "  * a note: here it shows how the retrieved context holds much more weight than the LLM's own knowledge (mainly because we requested in the prompt to \"ignore\" and only generate responses based on the retrieved context). This is a good example to notice because \"Dr. Watson\" is related to a well-known character in pop culture (from \"Sherlock Holmes\"), and the model likely has knowledge of this. However, as it’s using only the retrieved context, it won’t be confused with the other Dr. Watson — it will correctly respond by talking about the company’s founder.\n",
        "\n",
        "2. Now, upload the \"Attention-is-All-You-Need-Paper.pdf.\" You’ll notice it takes a bit longer to respond when asking a question since loading a document means we need to re-run all indexing and retrieval stages.\n",
        "Ask:\n",
        "\n",
        "* `what is attention?`\n",
        "  * again, this shows how the AI considers the retrieved context. Here, \"attention\" refers to the mechanism discussed in the paper, rather than the general meaning of the word \"attention.\""
      ],
      "metadata": {
        "id": "f8NQb0NFWBY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to improve 🚀\n",
        "\n",
        "By knowing exactly how each function operates and understanding the explanations covered throughout this project, you have the necessary knowledge to improve the results of your RAG application. Below we will list the strategies that can be applied to optimize the quality of the responses and the efficiency of the system:\n",
        "\n",
        "* Test other Embedding models - as mentioned, selecting the correct model for the RAG system is crucial, as it directly affects the accuracy of the responses, in addition to the use of resources and the scalability of the application. Choosing models that work well with the language and the task in question can significantly improve the results. By testing different models, you can identify which one offers the best combination of quality and efficiency for your needs.\n",
        "\n",
        "* Adjust the fixed (system) prompt - Modifying the system prompt to make it more explicit about the functions that the LLM should perform can improve the results. The prompt should clearly specify what the LLM should prioritize in the response and what should be ignored. This guides the model to focus on what is most relevant to your application and your goal.\n",
        "\n",
        "* Improve the user prompt - remind the user (perhaps by placing a warning in the interface) that the more specific the question is, the greater the chance of increasing the accuracy of the answers generated by LLM. The more detailed and clear the request, the more relevant the return will be. This practice also helps to reduce ambiguities that can harm the model's interpretation of the query.\n",
        "\n",
        "* Adjust the contextualization prompt - remember that this prompt reformulates the user's question based on the conversation history, something useful when the query needs context to be correctly interpreted. The contextualization prompt (context_q_system_prompt) instructs the model to take the history into account.\n",
        "\n",
        "* Test other LLMs - Exploring other language models, especially those that accept a larger number of tokens and perform well in the chosen language, can improve performance. For more demanding cases, it may be worth considering proprietary solutions such as ChatGPT or paid services (such as Groq, mentioned in Colab 1) that provide large open-source models. Larger models can better handle complex queries and provide more elaborate responses.\n",
        "\n",
        "* Adjust the retrieval parameters (k and fetch_k) - Modifying the parameters of the retrieval steps, such as the values ​​of k and fetch_k, can have a significant impact on the performance of your application. Try starting with smaller values ​​and increasing them as necessary, always monitoring the impact on the relevance and quality of the responses. For more details, see the section on the RAG pipeline and the retriever. Another idea would be to test other algorithms besides MMR.\n",
        "\n",
        "* Make it better prepared to accept any document - one idea is to preprocess PDF files (or other formats) to adapt them to the vector store. PDFs often have tables or other structures that make interpretation difficult; or documents in different formats such as HTML, CSV, or PPTX are not structured for optimal information extraction. Preparing these files is crucial to ensure that relevant content is correctly captured and made available to the retrieval system.\n",
        "* There are specialized solutions that automate this transformation, organizing the data and eliminating unnecessary information. This optimizes the workflow and improves the accuracy of the results. One example is the Unstructured service (Visit https://unstructured.io), which facilitates the extraction of complex data from files, making them ready for use in vector databases and LLM frameworks, which increases the quality of information retrieval and the performance of the RAG application.\n",
        "* To use this in langchain is simple, you can use the Document Loader method. In practice, just load the document using the Unstructured document loader (instead of the PyPDFLoader that we used). More details here: https://python.langchain.com/v0.2/docs/integrations/document_loaders/unstructured_file/\n",
        "\n",
        "These strategies aim to optimize the efficiency and quality of the RAG system's responses, adapting it to your specific use case."
      ],
      "metadata": {
        "id": "Ex1IJrK34UCw"
      }
    }
  ]
}