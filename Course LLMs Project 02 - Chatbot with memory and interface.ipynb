{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOaSQlfiFLID"
   },
   "source": [
    "# Project 02 - Custom Chatbot with Memory and User Interface\n",
    "\n",
    "> In this project, you will learn how to add improvements to the chat system we've created by implementing a memory system, allowing the bot to remember the conversation history. This will give it a better understanding of the message context based on past interactions (chatbots with this capability are known as \"Context-Aware Chatbots\"). Additionally, we will explore how to easily create a user-friendly interface (UI) for your application using a versatile library called Streamlit.\n",
    "\n",
    "We'll learn how to implement this using Colab and also how to adapt the code to run in your local environment, which might be more beneficial.\n",
    "\n",
    "To make our application more flexible, we will prepare it to work with different models and LLM providers, both open-source (running locally or via cloud) and proprietary (via API only).\n",
    "\n",
    "This way, if you're using Colab, the application will still function even if you select CPU instead of GPU.\n",
    "\n",
    "We'll discuss the advantages of each method later as we develop the integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GauNtm2ws46r"
   },
   "source": [
    "## [ ! ] How to Run Locally\n",
    "\n",
    "* To run the code for this project in a local environment, follow the instructions to install the necessary dependencies using the commands below. You can use the same installation commands. For more details, check out the video lessons on local setup with Streamlit.\n",
    "\n",
    "* You can already run it locally as shown in the lesson, but if you're encountering configuration errors in your local environment, we recommend using Colab first to avoid disrupting the learning flow. However, if you choose to do it locally right away, that's also very possible and actually it is the ideal, as Streamlit requires working with .py files, while in Colab we use .ipynb due to Jupyter Notebook. Therefore, you will need to combine everything into a single .py file.\n",
    "\n",
    "* Additionally, when running locally, you can see changes more quickly. After executing the command to initialize Streamlit (e.g., `!streamlit run proj02.py`), you just need to edit and save the .py script, then refresh the Streamlit page to see the updates. In other words, you donâ€™t need to re-run the `!streamlit`... command.\n",
    "\n",
    "Before running your code locally, make sure all the libraries listed in the pip install command are installed. If you haven't installed them yet, you can do so directly from the terminal in VS Code (if you're using that IDE) or from a regular terminal/prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_CDI-qUEI0m"
   },
   "source": [
    "## Installation and Configuration\n",
    "\n",
    "We need to install some libraries that will be necessary for our application, such as LangChain and Streamlit (to create the user interface), and some other necessary packages that we used previously\n",
    "\n",
    "> If you are running locally: you also need to install pytorch, if you do not have it installed already (remember that Colab already has it installed by default, just import it).\n",
    "* To avoid compatibility issues, we recommend this command: `pip install torch==2.3.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu121`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30752,
     "status": "ok",
     "timestamp": 1730233292203,
     "user": {
      "displayName": "Jones Granatyr",
      "userId": "10042675233362078631"
     },
     "user_tz": 180
    },
    "id": "CXD-F395R75e",
    "outputId": "c44a8caa-d3ce-4a09-964f-4a8ea41ab655"
   },
   "outputs": [],
   "source": [
    "! pip install -q streamlit langchain sentence-transformers\n",
    "! pip install -q langchain_community langchain-huggingface langchain_ollama langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0qSu4O8tXIh"
   },
   "source": [
    "> Installing Localtunnel\n",
    "\n",
    "If you are running Colab, you also need to install Localtunnel so that we can connect to the application generated with Streamlit.\n",
    "\n",
    "This will be explained in the step where the interface is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5851,
     "status": "ok",
     "timestamp": 1730233307432,
     "user": {
      "displayName": "Jones Granatyr",
      "userId": "10042675233362078631"
     },
     "user_tz": 180
    },
    "id": "ZdYajOzC-1Y6",
    "outputId": "e69ad57e-2b95-4323-a4c1-6ad3a8a5c3ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'npm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! npm install localtunnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQahRcTktkWu"
   },
   "source": [
    "### Loading environment variables with dotenv\n",
    "\n",
    "We will use the **dotenv** library, which simplifies the management of environment variables by storing them in a .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3760,
     "status": "ok",
     "timestamp": 1730233333029,
     "user": {
      "displayName": "Jones Granatyr",
      "userId": "10042675233362078631"
     },
     "user_tz": 180
    },
    "id": "7qWrf4IMAjrz",
    "outputId": "7f73e072-dc05-44e2-abd6-6d80222d08d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in d:\\gen-ai\\lab-scenarios\\.conda\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82R6hEnbG98h"
   },
   "source": [
    "#### Creating the .env file\n",
    "\n",
    "The `%%writefile` command allows the notebook cell to be saved as an external file, with the specified name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1730233405121,
     "user": {
      "displayName": "Jones Granatyr",
      "userId": "10042675233362078631"
     },
     "user_tz": 180
    },
    "id": "UusxhnSpA4lL",
    "outputId": "0764122a-4ceb-47dc-cf6d-fe41338ab493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "HUGGINGFACE_API_KEY=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\n",
    "HUGGINGFACEHUB_API_TOKEN=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\n",
    "OPENAI_API_KEY=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\n",
    "TAVILY_API_KEY=##########\n",
    "SERPAPI_API_KEY=##########\n",
    "LANGCHAIN_API_KEY=##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPy_VbTAbBmQ"
   },
   "source": [
    "## (code explanations - step by step)\n",
    "\n",
    "First, we will do all the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7086,
     "status": "ok",
     "timestamp": 1730233437036,
     "user": {
      "displayName": "Jones Granatyr",
      "userId": "10042675233362078631"
     },
     "user_tz": 180
    },
    "id": "-R1bnBsREveD",
    "outputId": "93325507-f0e7-419e-b0dc-a523cef41736"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import torch\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "from langchain_huggingface  import HuggingFaceEndpoint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPNTtsRophKl"
   },
   "source": [
    "### Explanations about model provider selection\n",
    "\n",
    "We are designing our application to allow model loading from different model providers:\n",
    "\n",
    "1. Hugging Face Hub\n",
    "\n",
    " * Inference runs on an external server, so you don't need to worry about local environment setup.\n",
    " * When to use: Great for those who need fast inference with open-source models and no cost, running on simpler setups without needing a GPU. Note that for very large models, a Pro subscription on Hugging Face is required (though not needed for the models we use in this course).\n",
    "\n",
    "2. OpenAI (ChatGPT)\n",
    "\n",
    " * Runs via API and requires an internet connection.\n",
    " * When to use: Ideal for users who donâ€™t want to deal with setup or lack the hardware to run models locally. It's also suited for those willing to pay a few cents for every 1M tokens generated (in the end, itâ€™s a low cost for extensive use. You can check pricing on OpenAI's pricing page).\n",
    "\n",
    "3. Ollama\n",
    "\n",
    " * Runs locally, optimized for local environments.\n",
    " * When to use: Perfect for users looking for a free solution and have the necessary hardware, or for those who want to work offline. Currently, this is the recommended option for local use. Thatâ€™s why we arenâ€™t using the regular Hugging Face pipeline (the one we learned first in this course), but you can still implement it later, especially if youâ€™re using a GPU on Colab or have your own GPU.\n",
    "\n",
    "With this in mind, we can proceed to create the model loading function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46TNqYW5qFnY"
   },
   "source": [
    "### Model loading function\n",
    "\n",
    "> `[ ! ]` Weâ€™ll define a variable now to make it easier to switch between methods later (can be useful either to compare different models or when you just want to use another model).\n",
    "\n",
    "Note: The `# @param` is just a way to easily parameterize the code in Colab. By changing the value in the field next to it, the corresponding value in the code will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQpkbSw4bfek"
   },
   "outputs": [],
   "source": [
    "model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikJ1kJkqqiGP"
   },
   "source": [
    "> **Loading Functions**\n",
    "\n",
    "We'll organize the code into a function to keep it more structured and organized.\n",
    "\n",
    "The loading code wonâ€™t be explained here, as itâ€™s the same code used in the first Colab, we just copied and placed into functions.\n",
    "\n",
    "`[!] Note:` If you want to make your application more dynamic, you can customize the function to accept more parameters. Weâ€™ve only included the model name and temperature for now, as we donâ€™t need to vary other attributes, but if you want to make your program more flexible, you can modify the function to accept additional parameters.\n",
    "\n",
    "> **Model Selection**\n",
    "\n",
    "Weâ€™ll set it up so that by default, the Meta-Llama-3-8B-Instruct model is loaded, as weâ€™ve already confirmed it works well for conversations in Portuguese. If we donâ€™t pass the model parameter (which corresponds to the model name) to this function, it will load this model by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Doy-G4Sq02f"
   },
   "outputs": [],
   "source": [
    "def model_hf_hub(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", temperature=0.1):\n",
    "  llm = HuggingFaceEndpoint(\n",
    "      repo_id=model,\n",
    "      temperature=temperature,\n",
    "      max_new_tokens=512,\n",
    "      return_full_text=False,\n",
    "      #model_kwargs={\n",
    "      #    \"max_length\": 64,\n",
    "      #    #\"stop\": [\"<|eot_id|>\"],\n",
    "      #}\n",
    "  )\n",
    "  return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U8twbz8q5rT"
   },
   "source": [
    "Below, we write a function for the other model loading methods\n",
    "\n",
    "Again, the loading code will not be explained here because it is the same code used in the first Colab, it was just copied and placed inside functions. During the lesson, you can simply open the Colab link and copy and paste it into your project, there is no need to code manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgMAPA2jrBZk"
   },
   "outputs": [],
   "source": [
    "def model_openai(model=\"gpt-4o-mini\", temperature=0.1):\n",
    "    llm = ChatOpenAI(\n",
    "        model=model,\n",
    "        temperature=temperature\n",
    "        # other parameters...\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def model_ollama(model=\"phi3\", temperature=0.1):\n",
    "    llm = ChatOllama(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSgpW0wCrA8d"
   },
   "source": [
    "> **`[ ! ]`** Here you could set functions for other services too, like Groq or Google for example (see the end of the 1st Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "497W8gY2rTlz"
   },
   "source": [
    "### Defining the process: Prompt, chain and template response\n",
    "\n",
    "In this step we will basically:\n",
    "1. define the prompt template for our virtual assistant\n",
    " * and also see how we can further customize the prompt so that it is better suited to our objective\n",
    "2. load the template\n",
    "3. create the execution chain (prompt + llm + text post-processing)\n",
    "\n",
    "Let's wrap this inside a function that will return the model response (here we call it model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdJdh4EbrrZo"
   },
   "outputs": [],
   "source": [
    "def model_response(user_query, chat_history, model_class):\n",
    "\n",
    "    ## Loading the LLM\n",
    "    if model_class == \"hf_hub\":\n",
    "        llm = model_hf_hub()\n",
    "    elif model_class == \"openai\":\n",
    "        llm = model_openai()\n",
    "    elif model_class == \"ollama\":\n",
    "        llm = model_ollama()\n",
    "\n",
    "    ## Prompt definition\n",
    "    system_prompt = \"\"\"\n",
    "    You are a helpful assistant answering general questions. Please respond in {language}.\n",
    "    \"\"\"\n",
    "    # corresponds to the language we want in our output\n",
    "    language = \"the same language the user is using to chat\" #or, force to answer in a specific language e.g. english\n",
    "\n",
    "    # Adapting to the pipeline (for open source model with hugging face)\n",
    "    if model_class.startswith(\"hf\"):\n",
    "        user_prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    else:\n",
    "        user_prompt = \"{input}\"\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", user_prompt)\n",
    "    ])\n",
    "\n",
    "    ## Creating the chain\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    ## Response return / Stream\n",
    "    return chain.stream({\n",
    "        \"chat_history\": chat_history,\n",
    "        \"input\": user_query,\n",
    "        \"language\": language\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylXqEYEztMsG"
   },
   "source": [
    "> **Detailed explanation**\n",
    "\n",
    "**Loading the LLM**\n",
    "* Depending on the value of model_class, initializes the corresponding language model, choosing the appropriate function (which we defined before)\n",
    "\n",
    "**Prompt definition**\n",
    "(`prompt_template = ChatPromptTemplate.from_messages...`)\n",
    "* This function generates the chatbot response based on the user query (user_query), the chat history (chat_history) and the chosen language model (model_class).\n",
    " * system_prompt: Defines the system message that instructs the assistant to\n",
    "respond in the same language the user is using to chat, but here you could change the value for variable `language` to be english or any other language, if you want to make sure the LLM will answer in that idiom (so, it's just to reinforce and ensure that it will always respond in that language, but the model is clever enough in general to understand, so that's optional).\n",
    " * user_prompt: Format of the user input that will be used in the prompt.\n",
    " * prompt_template: Creates a prompt template that combines the system_prompt, the chat history and the user_prompt.\n",
    "for this, the `MessagesPlaceholder` method is used, which is used to have full control over which messages will be rendered during formatting. This can be useful when you are not sure which function to use for your message prompt templates or when you want to insert a list of messages during formatting (our case).\n",
    "\n",
    "* about `if model_class.startswith(\"hf\"):`\n",
    " * If we are loading using the Hugging Face pipeline, it is necessary to make this adjustment. (as explained in colab 1, in the section that talks about open source model templates).\n",
    "  * if so, leave the prompt in this template. the `.startswith` is a quick way to check: if model_class starts with \"hf\" (because we define the hugging face pipeline functions with that name) then it is an HF pipeline and therefore applies the template\n",
    "  * otherwise, just use the normal prompt\n",
    " * Note: at the moment, it is necessary to do this with langchain. If this becomes 100% unnecessary in the future, we will remove this section from here\n",
    "\n",
    "**Chain Creation**\n",
    "\n",
    "* Combines the prompt template, the chosen language model and an output parser (StrOutputParser) into a chain.\n",
    "\n",
    "**Returning the response / Stream** (`return chain.stream...`)\n",
    "\n",
    "* Executes the execution chain (chain) with the provided parameters and returns the response generated by the model in a continuous manner, known as streaming\n",
    "\n",
    "* Basically, this consists of streaming back each token as it is generated. This allows the user to see the progress and not wait for a blank screen until the processing is 100% complete. (more explanations about streaming in Colab 1)\n",
    "\n",
    "* Note: at the moment this mode is not visible using the huggingface pipelined implementation, so it will be visible if you select \"openai\" or \"ollama\" (or others, which support this feature). In other words, it will return the text all at once. But using the hugging face hub we have a very fast response, so it is not a problem, since the inference is faster than normal so we can quickly visualize the result.\n",
    "\n",
    "* To change to stream mode was quite simple: we changed from `chain.invoke` (normal mode we used before) to `chain.stream`. Since chains are runnables they benefit from the [runnable interface](https://python.langchain.com/docs/expression_language/interface/), we can do this in a very practical way, since it already has support for this mode.\n",
    "* (it would be nice if you ran with `chain.invoke` first and then with `chain.stream`, just for comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyDn6BUTwcYE"
   },
   "source": [
    "### Session state management\n",
    "\n",
    "In this section, we will be initializing the conversation history, using the session state.\n",
    "\n",
    "What the code below does is check if the chat_history is not present in the session state (st.session_state). If it is not, it initializes it with a welcome message from the virtual assistant. In `content` we declare this message, which will be the one with which our assistant will start the conversation\n",
    "\n",
    "* More about the session state:\n",
    "\n",
    "https://docs.streamlit.io/develop/api-reference/caching-and-state/st.session_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUkv8ZBewZd5"
   },
   "outputs": [],
   "source": [
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = [\n",
    "        AIMessage(content=\"Hi, I'm your virtual assistant! How can I help you?\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blstOa2RxE-P"
   },
   "source": [
    "### Defining the conversation\n",
    "\n",
    "Now we will render the message history.\n",
    "\n",
    "The snippet below iterates over the chat_history and displays each message in the Streamlit interface, differentiating between messages from the AI â€‹â€‹(AIMessage) and from the user (HumanMessage).\n",
    "\n",
    "* This loop goes through all the messages stored in the chat history, which is saved in the variable `st.session_state.chat_history`. This history contains both the messages sent by the user and the responses generated by the AI.\n",
    "* The code condition `if isinstance...` checks whether the current message (message) was generated by the AI. To do this, it uses the isinstance function, which checks whether the message object is an instance of the AIMessage class.\n",
    "  * If the message was generated by the AI, this code block creates a new \"bubble talk\" (the message box) that will be identified as \"AI\" using st.chat_message(\"AI\"). Inside this bubble, the message content is displayed in the interface using st.write.\n",
    "  * If the message was not generated by LLM, the code checks whether it was sent by the user. This is done by checking whether message is an instance of the HumanMessage class. In this case, it creates a \"bubble\" identified as \"Human\" with st.chat_message(\"Human\"), whose content of the message sent by the user will be displayed in the interface using st.write again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8mLYF0mxxT4"
   },
   "outputs": [],
   "source": [
    "for message in st.session_state.chat_history:\n",
    "    if isinstance(message, AIMessage):\n",
    "        with st.chat_message(\"AI\"):\n",
    "            st.write(message.content)\n",
    "    elif isinstance(message, HumanMessage):\n",
    "        with st.chat_message(\"Human\"):\n",
    "            st.write(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jorezWikx0wb"
   },
   "source": [
    "### User Input\n",
    "\n",
    "This code snippet is responsible for capturing user input, updating the chat history with the new message, displaying the messages in the interface, and generating a response from the AI.\n",
    "\n",
    "* `st.chat_input` creates an input field in the Streamlit interface where the user can type their message. The text typed by the user is stored in the user_query variable.\n",
    " * The second line of code is a condition that checks whether the user actually typed something in the input field. The check `user_query is not None` ensures that the field is not empty, and `user_query != \"\"` ensures that the string is not empty. If both conditions are true, the code inside the if block will be executed.\n",
    " * The 3rd line: the message typed by the user (user_query) is converted to a HumanMessage object and added to the chat history, `st.session_state.chat_history`. This keeps track of the messages sent by the user. * `with st.chat_message(\"Human\")` - we create a message in the conversation labeled \"Human\" in the interface to display the user's message. The st.markdown function is used to format and display the message (user_query) in the Streamlit interface.\n",
    " * `st.chat_message(\"AI\")` - we create a message labeled \"AI\" in the interface to display the response. Here the model_response() function is called to generate the AI â€‹â€‹response based on the user input (user_query), the chat history (st.session_state.chat_history), and the specified model class (model_class).\n",
    " * The AI â€‹â€‹response is generated continuously using `st.write_stream`, which streams the response in a convenient way, i.e., has the response displayed as it is generated. The chat history is then printed to the console for debugging.\n",
    " * Finally, the AI â€‹â€‹response (resp) is converted to an AIMessage object and added to the chat history, `st.session_state.chat_history`. This keeps track of the messages generated by the AI â€‹â€‹and completes the interaction cycle in the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzJGzrivye0k"
   },
   "outputs": [],
   "source": [
    "user_query = st.chat_input(\"Enter your message here...\")\n",
    "if user_query is not None and user_query != \"\":\n",
    "    st.session_state.chat_history.append(HumanMessage(content=user_query))\n",
    "\n",
    "    with st.chat_message(\"Human\"):\n",
    "        st.markdown(user_query)\n",
    "\n",
    "    with st.chat_message(\"AI\"):\n",
    "        resp = st.write_stream(model_response(user_query, st.session_state.chat_history, model_class))\n",
    "        print(st.session_state.chat_history)\n",
    "\n",
    "    st.session_state.chat_history.append(AIMessage(content=resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO2t7UV3HHiN"
   },
   "source": [
    "Now, we have all the code necessary for the functioning of our application's logic.\n",
    "\n",
    "## Launching the Interface\n",
    "\n",
    "In the next step, we just need to define some Streamlit settings and then gather all the code in a .py file, this way we can run it in Colab too.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpMahOoFzi1W"
   },
   "source": [
    "#### - Streamlit Settings\n",
    "\n",
    "Let's set up a few simple configs before we start our application. Streamlit supports a lot of other settings, but for now we'll keep it simple.\n",
    "\n",
    "```\n",
    "st.set_page_config(page_title=\"Your AI assistant ðŸ¤–\", page_icon=\"ðŸ¤–\")\n",
    "st.title(\"Your AI assistant ðŸ¤–\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* `st.set_page_config()` - This line defines the page configuration in Streamlit, specifying the browser tab title as the first parameter and the page icon (called a favicon) as the second. For the title, we initially leave it as \"Your AI assistant ðŸ¤–\" and the icon as a robot emoji.\n",
    "* `st.title()` - This line defines the main title of the application interface, which will be displayed prominently at the top of the page. We will set the same title as the page in the browser tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ9jZQsxzhRU"
   },
   "source": [
    "#### - Creating the final script\n",
    "\n",
    "Before we can start our application, we need to gather all of this code into a single .py script.\n",
    "\n",
    "We use `%%writefile proj02.py` in Google Colab to save the code to a file called proj02.py. This is necessary because when working in Colab, we usually write code directly into notebook cells. However, to run a Streamlit application, the code needs to be in a Python (.py) file. So the %%writefile command allows the notebook cell to be saved as an external file.\n",
    "\n",
    "Gathering the final code into a single .py file is essential for using Streamlit in Colab because the `!streamlit run app.py` command expects all of the application code to be in a single file. This makes it easier to run the application because Streamlit can load and execute all of the code at once, without having to deal with multiple cells or scattered files. It is a convenient way to consolidate the code so that Streamlit works properly in the Colab environment.\n",
    "\n",
    "Therefore, the block below was assembled by copying all the code we generated before (with the exception of the installation commands, which start with `!`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1730233511999,
     "user": {
      "displayName": "Jones Granatyr",
      "userId": "10042675233362078631"
     },
     "user_tz": 180
    },
    "id": "gC59Y_AF9Lu-",
    "outputId": "d73da7d8-ee8c-431a-e9dd-57b4fbb24698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing proj02.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile proj02.py\n",
    "\n",
    "import streamlit as st\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import torch\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Streamlit Settings\n",
    "st.set_page_config(page_title=\"Your AI assistant ðŸ¤–\", page_icon=\"ðŸ¤–\")\n",
    "st.title(\"Your AI assistant ðŸ¤–\")\n",
    "\n",
    "model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n",
    "\n",
    "## Model Providers\n",
    "def model_hf_hub(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", temperature=0.1):\n",
    "  llm = HuggingFaceEndpoint(\n",
    "      repo_id=model,\n",
    "      temperature=temperature,\n",
    "      max_new_tokens=512,\n",
    "      return_full_text=False,\n",
    "      #model_kwargs={\n",
    "      #    \"max_length\": 64,\n",
    "      #    #\"stop\": [\"<|eot_id|>\"],\n",
    "      #}\n",
    "  )\n",
    "  return llm\n",
    "\n",
    "def model_openai(model=\"gpt-4o-mini\", temperature=0.1):\n",
    "    llm = ChatOpenAI(\n",
    "        model=model,\n",
    "        temperature=temperature\n",
    "        # other parameters...\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def model_ollama(model=\"phi3\", temperature=0.1):\n",
    "    llm = ChatOllama(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "def model_response(user_query, chat_history, model_class):\n",
    "\n",
    "    ## Loading the LLM\n",
    "    if model_class == \"hf_hub\":\n",
    "        llm = model_hf_hub()\n",
    "    elif model_class == \"openai\":\n",
    "        llm = model_openai()\n",
    "    elif model_class == \"ollama\":\n",
    "        llm = model_ollama()\n",
    "\n",
    "    ## Prompt definition\n",
    "    system_prompt = \"\"\"\n",
    "    You are a helpful assistant answering general questions. Please respond in {language}.\n",
    "    \"\"\"\n",
    "    # corresponds to the language we want in our output\n",
    "    language = \"the same language the user is using to chat\" #or, force to answer in a specific language e.g. english\n",
    "\n",
    "    # Adapting to the pipeline (for open source model with hugging face)\n",
    "    if model_class.startswith(\"hf\"):\n",
    "        user_prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    else:\n",
    "        user_prompt = \"{input}\"\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", user_prompt)\n",
    "    ])\n",
    "\n",
    "    ## Creating the Chain\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    ## Response return / Stream\n",
    "    return chain.stream({\n",
    "        \"chat_history\": chat_history,\n",
    "        \"input\": user_query,\n",
    "        \"language\": language\n",
    "    })\n",
    "\n",
    "\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = [\n",
    "        AIMessage(content=\"Hi, I'm your virtual assistant! How can I help you?\"),\n",
    "    ]\n",
    "\n",
    "for message in st.session_state.chat_history:\n",
    "    if isinstance(message, AIMessage):\n",
    "        with st.chat_message(\"AI\"):\n",
    "            st.write(message.content)\n",
    "    elif isinstance(message, HumanMessage):\n",
    "        with st.chat_message(\"Human\"):\n",
    "            st.write(message.content)\n",
    "\n",
    "user_query = st.chat_input(\"Enter your message here...\")\n",
    "if user_query is not None and user_query != \"\":\n",
    "    st.session_state.chat_history.append(HumanMessage(content=user_query))\n",
    "\n",
    "    with st.chat_message(\"Human\"):\n",
    "        st.markdown(user_query)\n",
    "\n",
    "    with st.chat_message(\"AI\"):\n",
    "        resp = st.write_stream(model_response(user_query, st.session_state.chat_history, model_class))\n",
    "        print(st.session_state.chat_history)\n",
    "\n",
    "    st.session_state.chat_history.append(AIMessage(content=resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jrLVCt24tDC"
   },
   "source": [
    "### Running Streamlit\n",
    "\n",
    "With our script ready, simply execute the command below to run our application through Streamlit.\n",
    "This will make the Streamlit application run in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1730233571798,
     "user": {
      "displayName": "Jones Granatyr",
      "userId": "10042675233362078631"
     },
     "user_tz": 180
    },
    "id": "xDzu4rHv9T3f"
   },
   "outputs": [],
   "source": [
    "!streamlit run proj02.py &>/content/logs.txt &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COvIzXDGvpjI"
   },
   "source": [
    "Note:\n",
    "* The `&` at the end allows Colab to continue executing other cells without waiting for the Streamlit application to finish.\n",
    "\n",
    "* when running locally, `&>/content/logs.txt &` is not necessary\n",
    "\n",
    "* here we use it because Colab does not display the information we need in the terminal, since we cannot view it through Colab (as it works in a different way and we do not have access to the terminal that is updated in real time - at least in the free version).\n",
    "\n",
    "* What this snippet does is add the command logs to a file called `logs.txt`\n",
    "\n",
    "> If you are accessing locally now, just access the link that will appear in the terminal (local URL or Network URL, if you are on another device on the same network).\n",
    "\n",
    "* For Colab, you need one more command to open our application (see below)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ggfAm5wCDBs"
   },
   "source": [
    "### Access with LocalTunnel\n",
    "\n",
    "Before connecting with localtunnel, you need to get the external IP, which will be used as the password when launching the application in this next step.\n",
    "\n",
    "There are two ways to do this:\n",
    "\n",
    "1) with the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1730233665422,
     "user": {
      "displayName": "Jones Granatyr",
      "userId": "10042675233362078631"
     },
     "user_tz": 180
    },
    "id": "h2HtjvqKGMMr",
    "outputId": "ad1b4f8b-b628-4f33-eac0-93c35e43dfdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.236.216.236\n"
     ]
    }
   ],
   "source": [
    "!wget -q -O - ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaA_NWZ4vxkE"
   },
   "source": [
    "2) Or, alternatively, do it this way:\n",
    "\n",
    "* Open the Colab side panel\n",
    "* Click on the logs.txt file. Here is what would be displayed in the terminal\n",
    "* Select the IP number corresponding to the External URL. Only the IP number with the dots, without the http:// or port\n",
    " * For example: `35.184.1.10`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSkMYtN4zLnp"
   },
   "source": [
    "Now, just run the command below.\n",
    "\n",
    "This command uses npx localtunnel to \"expose\" the locally running Streamlit application to the internet. The application is hosted on port 8501, and localtunnel provides a public URL through which the application can be accessed.\n",
    "\n",
    "Then, enter the link that appears in the output and enter the IP in the Tunnel Password field. Then, click the button and wait for the interface to initialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104001,
     "status": "ok",
     "timestamp": 1730233779379,
     "user": {
      "displayName": "Jones Granatyr",
      "userId": "10042675233362078631"
     },
     "user_tz": 180
    },
    "id": "5EqkzAQE-5rp",
    "outputId": "089fcc6b-73f3-4a46-8055-2e355e5e8ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your url is: https://fine-bees-move.loca.lt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaj0JGZ4w0I-"
   },
   "source": [
    "Note:\n",
    "* If you get an error, reload the page and wait a few more moments.\n",
    "* If you are using a method that is not via API, it is normal for it to take a little longer on the first run.\n",
    "* If speed is a very important factor, we recommend using solutions where processing is done on an external server and connects via API, such as HF, Open AI or Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jsko_6V374kN"
   },
   "source": [
    "## Testing the application\n",
    "\n",
    "> Just a suggestion of what to type to test if it can understand the context of the conversation (send the messages in this order, 1 per line)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "What is the largest planet in the solar system?\n",
    "and the smallest?\n",
    "Thanks for the answers!\n",
    "Can you read our entire conversation history?\n",
    "What was the first question I asked?\n",
    "and the second?\n",
    "Generate a code in Python that writes the Fibonnaci sequence\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojxqLY40kZ_Q"
   },
   "source": [
    "---\n",
    "## Creating your own prompt\n",
    "> **Bonus tip: Structure for creating your own prompt**\n",
    "\n",
    "You can modify the prompt as you wish to suit your purpose. You can use this format:\n",
    "\n",
    "* Introduction: Start with a brief introduction to the topic, defining the basic concept.\n",
    "\n",
    "* Explanation: Provide a detailed but simple explanation of the concept. Use practical examples or analogies when necessary to facilitate understanding.\n",
    "\n",
    "* Steps or Components: If the concept has several components or steps, list and explain each one concisely.\n",
    "\n",
    "* Applications: Give examples of how this concept is applied in practice or in real contexts.\n",
    "\n",
    "* Summary: Conclude with a summary of the main ideas presented.\n",
    "\n",
    "* Additional Guidance: If relevant, offer additional tips or guidance for further exploration of the topic.\n",
    "\n",
    "Relevant keywords to add to your prompt and inform how you want your answer to be:\n",
    "* Clear, Objective, Simple, Practical example, Analogy, Detailed explanation, Summary\n",
    "\n",
    "Other ideas:\n",
    "* explain [x] to a layperson; explain it in an easy way as if you were explaining it to a child; explain like i'm five ...\n",
    "\n",
    "Going further:\n",
    "* you can also look for prompt frameworks to make LLMs perform their intended role in the best way possible. For example, the [COSTAR](https://medium.com/@frugalzentennial/unlocking-the-power-of-costar-prompt-engineering-a-guide-and-example-on-converting-goals-into-dc5751ce9875) framework, which ensures that all key aspects that influence an LLMâ€™s answer are considered, resulting in more personalized output responses.\n",
    "* When the goal is to make the model play a specific role or act in a certain way, it is called role-playing, and research on this has grown a lot (such as [this paper](https://arxiv.org/abs/2406.00627)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DL3ie4MprOha"
   },
   "source": [
    "## Alternative to Streamlit\n",
    "\n",
    "Creating our own application with Streamlit gives us a certain freedom, mainly because when creating \"from scratch\" we can leave it the way we want. But there are other more ready-made ways with the interface already created and available for use, not requiring dealing with code. Since our intention here is also to work with the source code and not depend solely on a program/interface, we did not end up addressing it, but if you are interested in using an alternative like this, then we have some recommendations:\n",
    "\n",
    "* Open WebUI - https://github.com/open-webui/open-webui\n",
    "* GPT4All - https://gpt4all.io/index.html\n",
    "* AnythingLLM - https://anythingllm.com\n",
    "\n",
    "These solutions have several other interesting features and integrations, so it may be a good idea to check them out if you are interested in exploring more LLMs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1JMjj2oIoHPSbheP_-E-YVb11IXmlD8Eq",
     "timestamp": 1730233110635
    },
    {
     "file_id": "1trGDlWUGJjHMGz3drzDBJ6s_vH7zUvzt",
     "timestamp": 1729594936783
    },
    {
     "file_id": "1Y-vZbs_sEa2Y5wq0jbqSoQ60w5OJEAv2",
     "timestamp": 1725397980979
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
